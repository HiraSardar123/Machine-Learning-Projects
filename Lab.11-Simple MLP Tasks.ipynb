{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mK68EzqU8vz"
      },
      "source": [
        "#**Lab 11: Simple MLP**\n",
        "**Submitted By**: Hira Sardar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Utgwz_-5xv2"
      },
      "source": [
        "#**Task 1: Implement Logical Gates Using the Perceptron Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7V8WOmg53ym"
      },
      "source": [
        "**Step 1: Define the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvDSThGx51CO"
      },
      "outputs": [],
      "source": [
        "# Define datasets for each gate\n",
        "def get_gate_data(gate_type):\n",
        "    if gate_type == \"AND\":\n",
        "        X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
        "        y = np.array([0, 0, 0, 1])  # Outputs\n",
        "    elif gate_type == \"OR\":\n",
        "        X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
        "        y = np.array([0, 1, 1, 1])  # Outputs\n",
        "    elif gate_type == \"NOT\":\n",
        "        X = np.array([[0], [1]])  # Single input for NOT\n",
        "        y = np.array([1, 0])  # Outputs\n",
        "    else:\n",
        "        raise ValueError(\"Invalid gate type!\")\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REpOMqUh6YJu"
      },
      "source": [
        "**Step 2: Design the simple Perceptron Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHnxBB3t6b87"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Perceptron class\n",
        "class SimplePerceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.01, epochs=50):\n",
        "        self.weights = np.random.randn(input_size)  # Initialize weights randomly\n",
        "        self.bias = np.random.randn()  # Initialize bias randomly\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    # Step activation function\n",
        "    def step_ftn(self, z):\n",
        "        return 1 if z > 0 else 0\n",
        "\n",
        "    # Forward pass to calculate output\n",
        "    def forward(self, x):\n",
        "        z = np.dot(self.weights, x) + self.bias\n",
        "        return self.step_ftn(z)\n",
        "\n",
        "    # Backward pass to update weights and bias\n",
        "    def backward(self, x, target, y_pred):\n",
        "        error = target - y_pred\n",
        "        # Update weights and bias using the learning rate and error\n",
        "        self.weights += self.learning_rate * error * x\n",
        "        self.bias += self.learning_rate * error\n",
        "\n",
        "    # Train function to iterate over the dataset and apply the forward & backward passes\n",
        "    def train(self, X, y, epochs=None):\n",
        "        if epochs is None:\n",
        "            epochs = self.epochs  # Default to self.epochs if not provided\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            error_count = 0  # Track misclassifications\n",
        "            for x, target in zip(X, y):\n",
        "                y_pred = self.forward(x)  # Forward pass to get prediction\n",
        "                error = target - y_pred  # Calculate error\n",
        "\n",
        "                if error != 0:\n",
        "                    self.backward(x, target, y_pred)  # Call the backward function to update weights and bias\n",
        "                    error_count += 1\n",
        "\n",
        "            # Print weights, bias, and errors after each epoch\n",
        "            print(f\"Epoch {epoch + 1}: Weights={self.weights}, Bias={self.bias}, Errors={error_count}\")\n",
        "\n",
        "            # Stop early if all inputs are classified correctly\n",
        "            if error_count == 0:\n",
        "                print(\"Training completed successfully!\")\n",
        "                break\n",
        "\n",
        "    def test(self, X, y):\n",
        "      correct_predictions = 0  # Initialize counter for correct predictions\n",
        "      total_predictions = len(y)  # Total number of samples\n",
        "\n",
        "      for inputs, target in zip(X, y):\n",
        "          output = self.forward(inputs)  # Get prediction\n",
        "          print(f\"Input: {inputs}, Target: {target}, Predicted Output: {output}\")\n",
        "\n",
        "          if output == target:\n",
        "              correct_predictions += 1  # Increment if prediction is correct\n",
        "\n",
        "      accuracy = (correct_predictions / total_predictions) * 100  # Calculate accuracy\n",
        "      print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "      return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyPhTewJ_J4H"
      },
      "source": [
        "**Step 3 and 4: Train and testthe Perceptron for Each Gate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMHM62eV_Msx",
        "outputId": "77b2abee-a0bf-4357-e1a1-9f7683beb01e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training perceptron for AND gate...\n",
            "Epoch 1: Weights=[-1.67030827  0.15309245], Bias=0.43788582999752534, Errors=3\n",
            "Epoch 2: Weights=[-1.66030827  0.15309245], Bias=0.42788582999752534, Errors=3\n",
            "Epoch 3: Weights=[-1.65030827  0.15309245], Bias=0.4178858299975253, Errors=3\n",
            "Epoch 4: Weights=[-1.64030827  0.15309245], Bias=0.4078858299975253, Errors=3\n",
            "Epoch 5: Weights=[-1.63030827  0.15309245], Bias=0.3978858299975253, Errors=3\n",
            "Epoch 6: Weights=[-1.62030827  0.15309245], Bias=0.3878858299975253, Errors=3\n",
            "Epoch 7: Weights=[-1.61030827  0.15309245], Bias=0.3778858299975253, Errors=3\n",
            "Epoch 8: Weights=[-1.60030827  0.15309245], Bias=0.3678858299975253, Errors=3\n",
            "Epoch 9: Weights=[-1.59030827  0.15309245], Bias=0.3578858299975253, Errors=3\n",
            "Epoch 10: Weights=[-1.58030827  0.15309245], Bias=0.34788582999752526, Errors=3\n",
            "Epoch 11: Weights=[-1.57030827  0.15309245], Bias=0.33788582999752526, Errors=3\n",
            "Epoch 12: Weights=[-1.56030827  0.15309245], Bias=0.32788582999752525, Errors=3\n",
            "Epoch 13: Weights=[-1.55030827  0.15309245], Bias=0.31788582999752524, Errors=3\n",
            "Epoch 14: Weights=[-1.54030827  0.15309245], Bias=0.30788582999752523, Errors=3\n",
            "Epoch 15: Weights=[-1.53030827  0.15309245], Bias=0.2978858299975252, Errors=3\n",
            "Epoch 16: Weights=[-1.52030827  0.15309245], Bias=0.2878858299975252, Errors=3\n",
            "Epoch 17: Weights=[-1.51030827  0.15309245], Bias=0.2778858299975252, Errors=3\n",
            "Epoch 18: Weights=[-1.50030827  0.15309245], Bias=0.2678858299975252, Errors=3\n",
            "Epoch 19: Weights=[-1.49030827  0.15309245], Bias=0.2578858299975252, Errors=3\n",
            "Epoch 20: Weights=[-1.48030827  0.15309245], Bias=0.24788582999752518, Errors=3\n",
            "Epoch 21: Weights=[-1.47030827  0.15309245], Bias=0.23788582999752517, Errors=3\n",
            "Epoch 22: Weights=[-1.46030827  0.15309245], Bias=0.22788582999752516, Errors=3\n",
            "Epoch 23: Weights=[-1.45030827  0.15309245], Bias=0.21788582999752515, Errors=3\n",
            "Epoch 24: Weights=[-1.44030827  0.15309245], Bias=0.20788582999752514, Errors=3\n",
            "Epoch 25: Weights=[-1.43030827  0.15309245], Bias=0.19788582999752513, Errors=3\n",
            "Epoch 26: Weights=[-1.42030827  0.15309245], Bias=0.18788582999752512, Errors=3\n",
            "Epoch 27: Weights=[-1.41030827  0.15309245], Bias=0.1778858299975251, Errors=3\n",
            "Epoch 28: Weights=[-1.40030827  0.15309245], Bias=0.1678858299975251, Errors=3\n",
            "Epoch 29: Weights=[-1.39030827  0.15309245], Bias=0.1578858299975251, Errors=3\n",
            "Epoch 30: Weights=[-1.38030827  0.15309245], Bias=0.1478858299975251, Errors=3\n",
            "Epoch 31: Weights=[-1.37030827  0.15309245], Bias=0.13788582999752508, Errors=3\n",
            "Epoch 32: Weights=[-1.36030827  0.15309245], Bias=0.12788582999752507, Errors=3\n",
            "Epoch 33: Weights=[-1.35030827  0.15309245], Bias=0.11788582999752507, Errors=3\n",
            "Epoch 34: Weights=[-1.34030827  0.15309245], Bias=0.10788582999752508, Errors=3\n",
            "Epoch 35: Weights=[-1.33030827  0.15309245], Bias=0.09788582999752508, Errors=3\n",
            "Epoch 36: Weights=[-1.32030827  0.15309245], Bias=0.08788582999752509, Errors=3\n",
            "Epoch 37: Weights=[-1.31030827  0.15309245], Bias=0.0778858299975251, Errors=3\n",
            "Epoch 38: Weights=[-1.30030827  0.15309245], Bias=0.0678858299975251, Errors=3\n",
            "Epoch 39: Weights=[-1.29030827  0.15309245], Bias=0.0578858299975251, Errors=3\n",
            "Epoch 40: Weights=[-1.28030827  0.15309245], Bias=0.047885829997525095, Errors=3\n",
            "Epoch 41: Weights=[-1.27030827  0.15309245], Bias=0.03788582999752509, Errors=3\n",
            "Epoch 42: Weights=[-1.26030827  0.15309245], Bias=0.02788582999752509, Errors=3\n",
            "Epoch 43: Weights=[-1.25030827  0.15309245], Bias=0.01788582999752509, Errors=3\n",
            "Epoch 44: Weights=[-1.24030827  0.15309245], Bias=0.007885829997525089, Errors=3\n",
            "Epoch 45: Weights=[-1.23030827  0.15309245], Bias=-0.002114170002474911, Errors=3\n",
            "Epoch 46: Weights=[-1.22030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 47: Weights=[-1.21030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 48: Weights=[-1.20030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 49: Weights=[-1.19030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 50: Weights=[-1.18030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 51: Weights=[-1.17030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 52: Weights=[-1.16030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 53: Weights=[-1.15030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 54: Weights=[-1.14030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 55: Weights=[-1.13030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 56: Weights=[-1.12030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 57: Weights=[-1.11030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 58: Weights=[-1.10030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 59: Weights=[-1.09030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 60: Weights=[-1.08030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 61: Weights=[-1.07030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 62: Weights=[-1.06030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 63: Weights=[-1.05030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 64: Weights=[-1.04030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 65: Weights=[-1.03030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 66: Weights=[-1.02030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 67: Weights=[-1.01030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 68: Weights=[-1.00030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 69: Weights=[-0.99030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 70: Weights=[-0.98030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 71: Weights=[-0.97030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 72: Weights=[-0.96030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 73: Weights=[-0.95030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 74: Weights=[-0.94030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 75: Weights=[-0.93030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 76: Weights=[-0.92030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 77: Weights=[-0.91030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 78: Weights=[-0.90030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 79: Weights=[-0.89030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 80: Weights=[-0.88030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 81: Weights=[-0.87030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 82: Weights=[-0.86030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 83: Weights=[-0.85030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 84: Weights=[-0.84030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 85: Weights=[-0.83030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 86: Weights=[-0.82030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 87: Weights=[-0.81030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 88: Weights=[-0.80030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 89: Weights=[-0.79030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 90: Weights=[-0.78030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 91: Weights=[-0.77030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 92: Weights=[-0.76030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 93: Weights=[-0.75030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 94: Weights=[-0.74030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 95: Weights=[-0.73030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 96: Weights=[-0.72030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 97: Weights=[-0.71030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 98: Weights=[-0.70030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 99: Weights=[-0.69030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 100: Weights=[-0.68030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 101: Weights=[-0.67030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 102: Weights=[-0.66030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 103: Weights=[-0.65030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 104: Weights=[-0.64030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 105: Weights=[-0.63030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 106: Weights=[-0.62030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 107: Weights=[-0.61030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 108: Weights=[-0.60030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 109: Weights=[-0.59030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 110: Weights=[-0.58030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 111: Weights=[-0.57030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 112: Weights=[-0.56030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 113: Weights=[-0.55030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 114: Weights=[-0.54030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 115: Weights=[-0.53030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 116: Weights=[-0.52030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 117: Weights=[-0.51030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 118: Weights=[-0.50030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 119: Weights=[-0.49030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 120: Weights=[-0.48030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 121: Weights=[-0.47030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 122: Weights=[-0.46030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 123: Weights=[-0.45030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 124: Weights=[-0.44030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 125: Weights=[-0.43030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 126: Weights=[-0.42030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 127: Weights=[-0.41030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 128: Weights=[-0.40030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 129: Weights=[-0.39030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 130: Weights=[-0.38030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 131: Weights=[-0.37030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 132: Weights=[-0.36030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 133: Weights=[-0.35030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 134: Weights=[-0.34030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 135: Weights=[-0.33030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 136: Weights=[-0.32030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 137: Weights=[-0.31030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 138: Weights=[-0.30030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 139: Weights=[-0.29030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 140: Weights=[-0.28030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 141: Weights=[-0.27030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 142: Weights=[-0.26030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 143: Weights=[-0.25030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 144: Weights=[-0.24030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 145: Weights=[-0.23030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 146: Weights=[-0.22030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 147: Weights=[-0.21030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 148: Weights=[-0.20030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 149: Weights=[-0.19030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 150: Weights=[-0.18030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 151: Weights=[-0.17030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 152: Weights=[-0.16030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 153: Weights=[-0.15030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 154: Weights=[-0.14030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 155: Weights=[-0.13030827  0.15309245], Bias=-0.002114170002474911, Errors=2\n",
            "Epoch 156: Weights=[-0.13030827  0.14309245], Bias=-0.012114170002474911, Errors=1\n",
            "Epoch 157: Weights=[-0.12030827  0.14309245], Bias=-0.012114170002474911, Errors=2\n",
            "Epoch 158: Weights=[-0.11030827  0.14309245], Bias=-0.012114170002474911, Errors=2\n",
            "Epoch 159: Weights=[-0.11030827  0.13309245], Bias=-0.02211417000247491, Errors=1\n",
            "Epoch 160: Weights=[-0.10030827  0.13309245], Bias=-0.02211417000247491, Errors=2\n",
            "Epoch 161: Weights=[-0.09030827  0.13309245], Bias=-0.02211417000247491, Errors=2\n",
            "Epoch 162: Weights=[-0.09030827  0.12309245], Bias=-0.032114170002474914, Errors=1\n",
            "Epoch 163: Weights=[-0.08030827  0.12309245], Bias=-0.032114170002474914, Errors=2\n",
            "Epoch 164: Weights=[-0.07030827  0.12309245], Bias=-0.032114170002474914, Errors=2\n",
            "Epoch 165: Weights=[-0.07030827  0.11309245], Bias=-0.042114170002474915, Errors=1\n",
            "Epoch 166: Weights=[-0.06030827  0.11309245], Bias=-0.042114170002474915, Errors=2\n",
            "Epoch 167: Weights=[-0.05030827  0.11309245], Bias=-0.042114170002474915, Errors=2\n",
            "Epoch 168: Weights=[-0.05030827  0.10309245], Bias=-0.05211417000247492, Errors=1\n",
            "Epoch 169: Weights=[-0.04030827  0.10309245], Bias=-0.05211417000247492, Errors=2\n",
            "Epoch 170: Weights=[-0.03030827  0.10309245], Bias=-0.05211417000247492, Errors=2\n",
            "Epoch 171: Weights=[-0.03030827  0.09309245], Bias=-0.06211417000247492, Errors=1\n",
            "Epoch 172: Weights=[-0.02030827  0.09309245], Bias=-0.06211417000247491, Errors=2\n",
            "Epoch 173: Weights=[-0.01030827  0.09309245], Bias=-0.06211417000247491, Errors=2\n",
            "Epoch 174: Weights=[-0.01030827  0.08309245], Bias=-0.07211417000247491, Errors=1\n",
            "Epoch 175: Weights=[-0.00030827  0.08309245], Bias=-0.07211417000247491, Errors=2\n",
            "Epoch 176: Weights=[0.00969173 0.08309245], Bias=-0.07211417000247491, Errors=2\n",
            "Epoch 177: Weights=[0.00969173 0.07309245], Bias=-0.08211417000247491, Errors=1\n",
            "Epoch 178: Weights=[0.00969173 0.07309245], Bias=-0.08211417000247491, Errors=0\n",
            "Training completed successfully!\n",
            "\n",
            "Testing AND gate:\n",
            "Input: [0 0], Target: 0, Predicted Output: 0\n",
            "Input: [0 1], Target: 0, Predicted Output: 0\n",
            "Input: [1 0], Target: 0, Predicted Output: 0\n",
            "Input: [1 1], Target: 1, Predicted Output: 1\n",
            "Accuracy: 100.00%\n",
            "Accuracy for AND gate: 100.00%\n",
            "\n",
            "Training perceptron for OR gate...\n",
            "Epoch 1: Weights=[-0.68650608  0.86473046], Bias=-0.5244419841406466, Errors=2\n",
            "Epoch 2: Weights=[-0.66650608  0.87473046], Bias=-0.5044419841406466, Errors=2\n",
            "Epoch 3: Weights=[-0.64650608  0.88473046], Bias=-0.48444198414064654, Errors=2\n",
            "Epoch 4: Weights=[-0.62650608  0.89473046], Bias=-0.4644419841406465, Errors=2\n",
            "Epoch 5: Weights=[-0.60650608  0.90473046], Bias=-0.4444419841406465, Errors=2\n",
            "Epoch 6: Weights=[-0.58650608  0.91473046], Bias=-0.4244419841406465, Errors=2\n",
            "Epoch 7: Weights=[-0.56650608  0.92473046], Bias=-0.40444198414064647, Errors=2\n",
            "Epoch 8: Weights=[-0.54650608  0.93473046], Bias=-0.38444198414064645, Errors=2\n",
            "Epoch 9: Weights=[-0.53650608  0.93473046], Bias=-0.37444198414064644, Errors=1\n",
            "Epoch 10: Weights=[-0.52650608  0.93473046], Bias=-0.36444198414064644, Errors=1\n",
            "Epoch 11: Weights=[-0.51650608  0.93473046], Bias=-0.3544419841406464, Errors=1\n",
            "Epoch 12: Weights=[-0.50650608  0.93473046], Bias=-0.3444419841406464, Errors=1\n",
            "Epoch 13: Weights=[-0.49650608  0.93473046], Bias=-0.3344419841406464, Errors=1\n",
            "Epoch 14: Weights=[-0.48650608  0.93473046], Bias=-0.3244419841406464, Errors=1\n",
            "Epoch 15: Weights=[-0.47650608  0.93473046], Bias=-0.3144419841406464, Errors=1\n",
            "Epoch 16: Weights=[-0.46650608  0.93473046], Bias=-0.3044419841406464, Errors=1\n",
            "Epoch 17: Weights=[-0.45650608  0.93473046], Bias=-0.2944419841406464, Errors=1\n",
            "Epoch 18: Weights=[-0.44650608  0.93473046], Bias=-0.28444198414064636, Errors=1\n",
            "Epoch 19: Weights=[-0.43650608  0.93473046], Bias=-0.27444198414064636, Errors=1\n",
            "Epoch 20: Weights=[-0.42650608  0.93473046], Bias=-0.26444198414064635, Errors=1\n",
            "Epoch 21: Weights=[-0.41650608  0.93473046], Bias=-0.25444198414064634, Errors=1\n",
            "Epoch 22: Weights=[-0.40650608  0.93473046], Bias=-0.24444198414064633, Errors=1\n",
            "Epoch 23: Weights=[-0.39650608  0.93473046], Bias=-0.23444198414064632, Errors=1\n",
            "Epoch 24: Weights=[-0.38650608  0.93473046], Bias=-0.2244419841406463, Errors=1\n",
            "Epoch 25: Weights=[-0.37650608  0.93473046], Bias=-0.2144419841406463, Errors=1\n",
            "Epoch 26: Weights=[-0.36650608  0.93473046], Bias=-0.2044419841406463, Errors=1\n",
            "Epoch 27: Weights=[-0.35650608  0.93473046], Bias=-0.19444198414064628, Errors=1\n",
            "Epoch 28: Weights=[-0.34650608  0.93473046], Bias=-0.18444198414064628, Errors=1\n",
            "Epoch 29: Weights=[-0.33650608  0.93473046], Bias=-0.17444198414064627, Errors=1\n",
            "Epoch 30: Weights=[-0.32650608  0.93473046], Bias=-0.16444198414064626, Errors=1\n",
            "Epoch 31: Weights=[-0.31650608  0.93473046], Bias=-0.15444198414064625, Errors=1\n",
            "Epoch 32: Weights=[-0.30650608  0.93473046], Bias=-0.14444198414064624, Errors=1\n",
            "Epoch 33: Weights=[-0.29650608  0.93473046], Bias=-0.13444198414064623, Errors=1\n",
            "Epoch 34: Weights=[-0.28650608  0.93473046], Bias=-0.12444198414064624, Errors=1\n",
            "Epoch 35: Weights=[-0.27650608  0.93473046], Bias=-0.11444198414064624, Errors=1\n",
            "Epoch 36: Weights=[-0.26650608  0.93473046], Bias=-0.10444198414064625, Errors=1\n",
            "Epoch 37: Weights=[-0.25650608  0.93473046], Bias=-0.09444198414064625, Errors=1\n",
            "Epoch 38: Weights=[-0.24650608  0.93473046], Bias=-0.08444198414064626, Errors=1\n",
            "Epoch 39: Weights=[-0.23650608  0.93473046], Bias=-0.07444198414064626, Errors=1\n",
            "Epoch 40: Weights=[-0.22650608  0.93473046], Bias=-0.06444198414064627, Errors=1\n",
            "Epoch 41: Weights=[-0.21650608  0.93473046], Bias=-0.054441984140646264, Errors=1\n",
            "Epoch 42: Weights=[-0.20650608  0.93473046], Bias=-0.04444198414064626, Errors=1\n",
            "Epoch 43: Weights=[-0.19650608  0.93473046], Bias=-0.03444198414064626, Errors=1\n",
            "Epoch 44: Weights=[-0.18650608  0.93473046], Bias=-0.02444198414064626, Errors=1\n",
            "Epoch 45: Weights=[-0.17650608  0.93473046], Bias=-0.014441984140646258, Errors=1\n",
            "Epoch 46: Weights=[-0.16650608  0.93473046], Bias=-0.004441984140646258, Errors=1\n",
            "Epoch 47: Weights=[-0.15650608  0.93473046], Bias=0.005558015859353742, Errors=1\n",
            "Epoch 48: Weights=[-0.14650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 49: Weights=[-0.13650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 50: Weights=[-0.12650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 51: Weights=[-0.11650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 52: Weights=[-0.10650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 53: Weights=[-0.09650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 54: Weights=[-0.08650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 55: Weights=[-0.07650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 56: Weights=[-0.06650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 57: Weights=[-0.05650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 58: Weights=[-0.04650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 59: Weights=[-0.03650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 60: Weights=[-0.02650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 61: Weights=[-0.01650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 62: Weights=[-0.00650608  0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 63: Weights=[0.00349392 0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 64: Weights=[0.01349392 0.93473046], Bias=0.005558015859353742, Errors=2\n",
            "Epoch 65: Weights=[0.01349392 0.93473046], Bias=-0.004441984140646258, Errors=1\n",
            "Epoch 66: Weights=[0.01349392 0.93473046], Bias=-0.004441984140646258, Errors=0\n",
            "Training completed successfully!\n",
            "\n",
            "Testing OR gate:\n",
            "Input: [0 0], Target: 0, Predicted Output: 0\n",
            "Input: [0 1], Target: 1, Predicted Output: 1\n",
            "Input: [1 0], Target: 1, Predicted Output: 1\n",
            "Input: [1 1], Target: 1, Predicted Output: 1\n",
            "Accuracy: 100.00%\n",
            "Accuracy for OR gate: 100.00%\n",
            "\n",
            "Training perceptron for NOT gate...\n",
            "Epoch 1: Weights=[-0.75921134], Bias=-0.7257677468184377, Errors=1\n",
            "Epoch 2: Weights=[-0.75921134], Bias=-0.7157677468184377, Errors=1\n",
            "Epoch 3: Weights=[-0.75921134], Bias=-0.7057677468184377, Errors=1\n",
            "Epoch 4: Weights=[-0.75921134], Bias=-0.6957677468184377, Errors=1\n",
            "Epoch 5: Weights=[-0.75921134], Bias=-0.6857677468184377, Errors=1\n",
            "Epoch 6: Weights=[-0.75921134], Bias=-0.6757677468184377, Errors=1\n",
            "Epoch 7: Weights=[-0.75921134], Bias=-0.6657677468184376, Errors=1\n",
            "Epoch 8: Weights=[-0.75921134], Bias=-0.6557677468184376, Errors=1\n",
            "Epoch 9: Weights=[-0.75921134], Bias=-0.6457677468184376, Errors=1\n",
            "Epoch 10: Weights=[-0.75921134], Bias=-0.6357677468184376, Errors=1\n",
            "Epoch 11: Weights=[-0.75921134], Bias=-0.6257677468184376, Errors=1\n",
            "Epoch 12: Weights=[-0.75921134], Bias=-0.6157677468184376, Errors=1\n",
            "Epoch 13: Weights=[-0.75921134], Bias=-0.6057677468184376, Errors=1\n",
            "Epoch 14: Weights=[-0.75921134], Bias=-0.5957677468184376, Errors=1\n",
            "Epoch 15: Weights=[-0.75921134], Bias=-0.5857677468184376, Errors=1\n",
            "Epoch 16: Weights=[-0.75921134], Bias=-0.5757677468184376, Errors=1\n",
            "Epoch 17: Weights=[-0.75921134], Bias=-0.5657677468184376, Errors=1\n",
            "Epoch 18: Weights=[-0.75921134], Bias=-0.5557677468184375, Errors=1\n",
            "Epoch 19: Weights=[-0.75921134], Bias=-0.5457677468184375, Errors=1\n",
            "Epoch 20: Weights=[-0.75921134], Bias=-0.5357677468184375, Errors=1\n",
            "Epoch 21: Weights=[-0.75921134], Bias=-0.5257677468184375, Errors=1\n",
            "Epoch 22: Weights=[-0.75921134], Bias=-0.5157677468184375, Errors=1\n",
            "Epoch 23: Weights=[-0.75921134], Bias=-0.5057677468184375, Errors=1\n",
            "Epoch 24: Weights=[-0.75921134], Bias=-0.4957677468184375, Errors=1\n",
            "Epoch 25: Weights=[-0.75921134], Bias=-0.4857677468184375, Errors=1\n",
            "Epoch 26: Weights=[-0.75921134], Bias=-0.4757677468184375, Errors=1\n",
            "Epoch 27: Weights=[-0.75921134], Bias=-0.46576774681843747, Errors=1\n",
            "Epoch 28: Weights=[-0.75921134], Bias=-0.45576774681843746, Errors=1\n",
            "Epoch 29: Weights=[-0.75921134], Bias=-0.44576774681843745, Errors=1\n",
            "Epoch 30: Weights=[-0.75921134], Bias=-0.43576774681843744, Errors=1\n",
            "Epoch 31: Weights=[-0.75921134], Bias=-0.42576774681843743, Errors=1\n",
            "Epoch 32: Weights=[-0.75921134], Bias=-0.4157677468184374, Errors=1\n",
            "Epoch 33: Weights=[-0.75921134], Bias=-0.4057677468184374, Errors=1\n",
            "Epoch 34: Weights=[-0.75921134], Bias=-0.3957677468184374, Errors=1\n",
            "Epoch 35: Weights=[-0.75921134], Bias=-0.3857677468184374, Errors=1\n",
            "Epoch 36: Weights=[-0.75921134], Bias=-0.3757677468184374, Errors=1\n",
            "Epoch 37: Weights=[-0.75921134], Bias=-0.3657677468184374, Errors=1\n",
            "Epoch 38: Weights=[-0.75921134], Bias=-0.35576774681843737, Errors=1\n",
            "Epoch 39: Weights=[-0.75921134], Bias=-0.34576774681843736, Errors=1\n",
            "Epoch 40: Weights=[-0.75921134], Bias=-0.33576774681843735, Errors=1\n",
            "Epoch 41: Weights=[-0.75921134], Bias=-0.32576774681843734, Errors=1\n",
            "Epoch 42: Weights=[-0.75921134], Bias=-0.31576774681843733, Errors=1\n",
            "Epoch 43: Weights=[-0.75921134], Bias=-0.3057677468184373, Errors=1\n",
            "Epoch 44: Weights=[-0.75921134], Bias=-0.2957677468184373, Errors=1\n",
            "Epoch 45: Weights=[-0.75921134], Bias=-0.2857677468184373, Errors=1\n",
            "Epoch 46: Weights=[-0.75921134], Bias=-0.2757677468184373, Errors=1\n",
            "Epoch 47: Weights=[-0.75921134], Bias=-0.2657677468184373, Errors=1\n",
            "Epoch 48: Weights=[-0.75921134], Bias=-0.2557677468184373, Errors=1\n",
            "Epoch 49: Weights=[-0.75921134], Bias=-0.24576774681843727, Errors=1\n",
            "Epoch 50: Weights=[-0.75921134], Bias=-0.23576774681843726, Errors=1\n",
            "Epoch 51: Weights=[-0.75921134], Bias=-0.22576774681843725, Errors=1\n",
            "Epoch 52: Weights=[-0.75921134], Bias=-0.21576774681843724, Errors=1\n",
            "Epoch 53: Weights=[-0.75921134], Bias=-0.20576774681843724, Errors=1\n",
            "Epoch 54: Weights=[-0.75921134], Bias=-0.19576774681843723, Errors=1\n",
            "Epoch 55: Weights=[-0.75921134], Bias=-0.18576774681843722, Errors=1\n",
            "Epoch 56: Weights=[-0.75921134], Bias=-0.1757677468184372, Errors=1\n",
            "Epoch 57: Weights=[-0.75921134], Bias=-0.1657677468184372, Errors=1\n",
            "Epoch 58: Weights=[-0.75921134], Bias=-0.1557677468184372, Errors=1\n",
            "Epoch 59: Weights=[-0.75921134], Bias=-0.14576774681843718, Errors=1\n",
            "Epoch 60: Weights=[-0.75921134], Bias=-0.13576774681843717, Errors=1\n",
            "Epoch 61: Weights=[-0.75921134], Bias=-0.12576774681843716, Errors=1\n",
            "Epoch 62: Weights=[-0.75921134], Bias=-0.11576774681843717, Errors=1\n",
            "Epoch 63: Weights=[-0.75921134], Bias=-0.10576774681843717, Errors=1\n",
            "Epoch 64: Weights=[-0.75921134], Bias=-0.09576774681843718, Errors=1\n",
            "Epoch 65: Weights=[-0.75921134], Bias=-0.08576774681843718, Errors=1\n",
            "Epoch 66: Weights=[-0.75921134], Bias=-0.07576774681843719, Errors=1\n",
            "Epoch 67: Weights=[-0.75921134], Bias=-0.0657677468184372, Errors=1\n",
            "Epoch 68: Weights=[-0.75921134], Bias=-0.05576774681843719, Errors=1\n",
            "Epoch 69: Weights=[-0.75921134], Bias=-0.04576774681843719, Errors=1\n",
            "Epoch 70: Weights=[-0.75921134], Bias=-0.03576774681843719, Errors=1\n",
            "Epoch 71: Weights=[-0.75921134], Bias=-0.025767746818437187, Errors=1\n",
            "Epoch 72: Weights=[-0.75921134], Bias=-0.015767746818437185, Errors=1\n",
            "Epoch 73: Weights=[-0.75921134], Bias=-0.005767746818437185, Errors=1\n",
            "Epoch 74: Weights=[-0.75921134], Bias=0.004232253181562815, Errors=1\n",
            "Epoch 75: Weights=[-0.75921134], Bias=0.004232253181562815, Errors=0\n",
            "Training completed successfully!\n",
            "\n",
            "Testing NOT gate:\n",
            "Input: [0], Target: 1, Predicted Output: 1\n",
            "Input: [1], Target: 0, Predicted Output: 0\n",
            "Accuracy: 100.00%\n",
            "Accuracy for NOT gate: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Train and test the perceptron for each gate\n",
        "for gate in [\"AND\", \"OR\", \"NOT\"]:\n",
        "    print(f\"\\nTraining perceptron for {gate} gate...\")\n",
        "    X, y = get_gate_data(gate)\n",
        "    input_size = X.shape[1]  # Number of features (inputs)\n",
        "    perceptron = SimplePerceptron(input_size=input_size, learning_rate=0.01, epochs=200)\n",
        "    perceptron.train(X, y)  # Train the perceptron\n",
        "\n",
        "    # Test perceptron after training and calculate accuracy\n",
        "    print(f\"\\nTesting {gate} gate:\")\n",
        "    accuracy = perceptron.test(X, y)  # Test and calculate accuracy\n",
        "    print(f\"Accuracy for {gate} gate: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBRFmmxeGAn2"
      },
      "source": [
        "**Step 5: Analysis of results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFDjkOLoGJcS"
      },
      "source": [
        "**How are the accuracies of each gateâ€™s prediction?**\n",
        "\n",
        "All 3 gates achieved a 100% accuracy.\n",
        "\n",
        "*Why 100% Accuracy?*\n",
        "\n",
        "1.  The gates (AND, OR, NOT) are relatively simple tasks that can be learned by the perceptron with a small number of training epochs.\n",
        "2.  The perceptron can correctly separate the output classes based on the linear nature of these gates (especially for AND and OR).\n",
        "3.  The training stops as soon as the perceptron makes no errors (i.e., all inputs are classified correctly), and this happens quite quickly for these gates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWBswZd5Igpg"
      },
      "source": [
        "**Can simple perception implement complex gates like XOR, comment?**\n",
        "\n",
        "A **simple perceptron** (with a single layer of output nodes and no hidden layers) **cannot implement complex gates like XOR**.\n",
        "\n",
        "1. **XOR Gate Behavior**:\n",
        "   The XOR gate has the following truth table:\n",
        "   ```\n",
        "   Input 1 | Input 2 | Output\n",
        "   ---------------------------\n",
        "      0     |    0    |    0\n",
        "      0     |    1    |    1\n",
        "      1     |    0    |    1\n",
        "      1     |    1    |    0\n",
        "   ```\n",
        "   The key observation is that the XOR gate is **not linearly separable**. This means that there is no straight line (or hyperplane in higher dimensions) that can separate the inputs that produce output `1` from the inputs that produce output `0`.\n",
        "\n",
        "2. **Linear Separability**:\n",
        "   - A perceptron works by computing a weighted sum of inputs and applying a threshold function (step function), which produces a linear decision boundary.\n",
        "   - For simple gates like **AND** and **OR**, you can find a linear boundary that separates the two classes (outputs `0` and `1`).\n",
        "   - For XOR, the points where the output is `1` (i.e., (0,1) and (1,0)) cannot be separated from the points where the output is `0` (i.e., (0,0) and (1,1)) by a single line. Hence, a single perceptron cannot learn the XOR function.\n",
        "\n",
        "3. **Solution for XOR**:\n",
        "   - The XOR gate **requires a more complex model**, typically a **multi-layer perceptron (MLP)** with at least one hidden layer. A multi-layer perceptron can combine multiple simple perceptrons to learn non-linear decision boundaries and thus implement complex logic like XOR.\n",
        "   - The use of a **non-linear activation function** (like ReLU or sigmoid) in the hidden layer allows the model to learn non-linear relationships between the inputs and outputs, which enables it to correctly classify XOR.\n",
        "\n",
        "### In Summary:\n",
        "- **Simple perceptron**: Can only handle linearly separable problems (like AND, OR, NOT).\n",
        "- **XOR problem**: Requires a **multi-layer perceptron** with hidden layers for non-linear separability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO-jxaQQIvjY"
      },
      "source": [
        "#**Task.02:MNIST Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T118ZY_WJQ7g"
      },
      "source": [
        "**Step 1: Load and preprocess the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSQflpzpLV1F"
      },
      "source": [
        "As the lab requires not using any library, I am downloading the dataset and dropping files in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGrrG2wHLrsE",
        "outputId": "c869b6a1-a68f-4540-83fe-682d2fbdcecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting idx2numpy\n",
            "  Downloading idx2numpy-1.2.3.tar.gz (6.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from idx2numpy) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from idx2numpy) (1.16.0)\n",
            "Building wheels for collected packages: idx2numpy\n",
            "  Building wheel for idx2numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idx2numpy: filename=idx2numpy-1.2.3-py3-none-any.whl size=7904 sha256=9e08855b458f30c6678f5504e92daed7fc6dbcc648d16aad4f27219d35c61714\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/f4/e7/643fc5f932ec2ff92997f43f007660feb23f948aa8486f1107\n",
            "Successfully built idx2numpy\n",
            "Installing collected packages: idx2numpy\n",
            "Successfully installed idx2numpy-1.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install idx2numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk8Y-RNMJDaY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import idx2numpy  # To load idx files\n",
        "def load_data():\n",
        "    # Load image and label data from the idx files\n",
        "    X_train = idx2numpy.convert_from_file('/content/train-images.idx3-ubyte')  # Shape: (60000, 28, 28)\n",
        "    y_train = idx2numpy.convert_from_file('/content/train-labels.idx1-ubyte')  # Shape: (60000,)\n",
        "    X_test = idx2numpy.convert_from_file('/content/t10k-images.idx3-ubyte')  # Shape: (10000, 28, 28)\n",
        "    y_test = idx2numpy.convert_from_file('/content/t10k-labels.idx1-ubyte')  # Shape: (10000,)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tscMPBa6L8O6"
      },
      "outputs": [],
      "source": [
        "# Normalize the data (scale pixel values to [0, 1])\n",
        "def preprocess_data(X_train, X_test):\n",
        "    X_train_normalized = X_train.astype('float32') / 255.0\n",
        "    X_test_normalized = X_test.astype('float32') / 255.0\n",
        "    return X_train_normalized, X_test_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5uzpPiCLyjp"
      },
      "outputs": [],
      "source": [
        "# Reshape the data to flatten the images (28x28 to 784)\n",
        "def reshape_data(X_train, X_test):\n",
        "    X_train_flattened = X_train.reshape(X_train.shape[0], -1)  # Flattening images\n",
        "    X_test_flattened = X_test.reshape(X_test.shape[0], -1)  # Flattening images\n",
        "    return X_train_flattened, X_test_flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT0IeXGCNWO0",
        "outputId": "ab04c7fd-a30d-456c-c39b-2f953aa3b26a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (48000, 784), Training labels shape: (48000,)\n",
            "Validation data shape: (12000, 784), Validation labels shape: (12000,)\n",
            "Test data shape: (10000, 784), Test labels shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into training and test sets (if needed)\n",
        "def split_data(X, y, test_size=0.2):\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "X_train_normalized, X_test_normalized = preprocess_data(X_train, X_test)\n",
        "X_train_flattened, X_test_flattened = reshape_data(X_train_normalized, X_test_normalized)\n",
        "\n",
        "\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = split_data(X_train_flattened, y_train)\n",
        "\n",
        "# Print shapes of the datasets to verify.\n",
        "print(f\"Training data shape: {X_train_split.shape}, Training labels shape: {y_train_split.shape}\")\n",
        "print(f\"Validation data shape: {X_val_split.shape}, Validation labels shape: {y_val_split.shape}\")\n",
        "print(f\"Test data shape: {X_test_flattened.shape}, Test labels shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0wFPSrdOKBP"
      },
      "source": [
        "**Step 2: Visualize the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "R1aT68LwOP5D",
        "outputId": "8a9b11be-6ea9-479c-b76e-e8a05f815d20"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAACMCAYAAAA9QmNpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAujUlEQVR4nO3debyN1f7A8e8xHWOOmWO8Ml9KZU6ICpfENRxXoqIUyjWXn5AohUgyq1xErlDmEjJkvG5CiQwZosxDhoPz/P7oZd21Fnvb55z9nLP3sz/v18vr9V3nu/fzrLPX2dPyrO+KchzHEQAAAAAAACDI0qR2BwAAAAAAAOBNTDwBAAAAAADAFUw8AQAAAAAAwBVMPAEAAAAAAMAVTDwBAAAAAADAFUw8AQAAAAAAwBVMPAEAAAAAAMAVTDwBAAAAAADAFUw8AQAAAAAAwBVhO/F08OBBiYqKkhEjRgTtmKtXr5aoqChZvXp10I6JxGFcvYux9SbG1ZsYV29iXL2JcfUuxtabGFdvYlz9S9GJp48//liioqJk69atKXnaFDN//nypX7++xMbGSnR0tBQqVEhatGghO3fuTO2uucrr4yoicvToUWnVqpXExMTIXXfdJU888YTs378/tbvlukgY2xUrVsjDDz8suXPnlpiYGKlSpYpMnz49tbvlKq+P66BBgyQqKuqWfxkzZkztrrnK6+P6008/Sffu3aVGjRqSMWNGiYqKkoMHD6Z2t1zn9XGdN2+exMXFSfHixSVz5sxSunRp6dmzp5w9eza1u+Yqr49rpL4Oi3h/bG/69NNPpXr16pIlSxaJiYmRGjVqyMqVK1O7W66JhHGdPXu23H///ZIxY0bJkyePdOjQQU6ePJna3XKV18c1lD47pUuVs3rUjh07JEeOHNKtWzfJnTu3HD9+XD788EOpUqWKbNiwQe69997U7iKS4OLFi/Lwww/LuXPnpF+/fpI+fXoZNWqU1K5dW7777jvJlStXancRSfTFF19I06ZNpXr16upD8pw5c6Rdu3Zy8uRJ6d69e2p3Eckwfvx4yZo1q2qnTZs2FXuD5NqwYYOMGTNGypUrJ2XLlpXvvvsutbuEIHj++eclNjZW2rZtK0WKFJEdO3bI2LFjZcmSJbJt2zbJlClTancRycDrsDcNGjRIBg8eLC1atJCnn35arl27Jjt37pSjR4+mdteQROPHj5fOnTtLvXr15N1335UjR47Ie++9J1u3bpVNmzZFxKSxF4XSZycmnoJowIABt/ysY8eOUqhQIRk/frxMmDAhFXqF5Bo3bpzs3btXNm/eLJUrVxYRkYYNG0r58uVl5MiR8uabb6ZyD5FUY8eOlQIFCsjKlSslOjpaREQ6deokZcqUkY8//piJpzDXokULyZ07d2p3A0HSpEkTOXv2rGTLlk1GjBjBxJNHzJ07V+rUqWP87IEHHpD27dvLzJkzpWPHjqnTMQQFr8Pes3HjRhk8eLCMHDmSz0keER8fL/369ZNatWrJV199JVFRUSIiUqNGDXn88cdl8uTJ8tJLL6VyL5EUofTZKeRqPMXHx8uAAQPkgQcekOzZs0uWLFnkoYceklWrVvm8z6hRo6Ro0aKSKVMmqV279m2Xtu3evVtatGghOXPmlIwZM0qlSpXkiy++uGN/Ll26JLt3707yZYZ58+aVzJkze/6S8TsJ53GdO3euVK5cWU06iYiUKVNG6tWrJ3PmzLnj/b0unMf2/PnzkiNHDjXpJCKSLl06yZ07d8T/L3s4j+tNjuPI+fPnxXGcgO/jdeE8rjlz5pRs2bLd8XaRKJzH1Z50EhFp1qyZiIj8+OOPd7y/l4XzuN7E6/DthfPYjh49WvLnzy/dunUTx3Hk4sWLd7xPpAjXcd25c6ecPXtW4uLi1KSTiEjjxo0la9asMnv27Duey8vCdVxFQuuzU8hNPJ0/f16mTJkiderUkbffflsGDRokJ06ckPr16992hu5f//qXjBkzRrp06SKvvvqq7Ny5U+rWrSu//fabus2uXbukWrVq8uOPP8orr7wiI0eOlCxZskjTpk1l/vz5fvuzefNmKVu2rIwdOzbg3+Hs2bNy4sQJ2bFjh3Ts2FHOnz8v9erVC/j+XhSu45qQkCDff/+9VKpU6ZZclSpVZN++fXLhwoXAHgSPCtexFfnzC8+uXbvktddek59//ln27dsnb7zxhmzdulX69OmT6MfCS8J5XG8qXry4ZM+eXbJlyyZt27Y1+hKpvDCuuJXXxvX48eMiIhF/pYwXxpXX4dsL57H9+uuvpXLlyjJmzBjJkyePZMuWTQoUKMDruITvuF69elVE5Lb/6ZopUyb573//KwkJCQE8At4UruMacpwU9NFHHzki4mzZssXnba5fv+5cvXrV+NmZM2ecfPnyOc8++6z62YEDBxwRcTJlyuQcOXJE/XzTpk2OiDjdu3dXP6tXr55ToUIF58qVK+pnCQkJTo0aNZySJUuqn61atcoREWfVqlW3/GzgwIEB/56lS5d2RMQRESdr1qxO//79nRs3bgR8/3Dj5XE9ceKEIyLO4MGDb8l98MEHjog4u3fv9nuMcOblsXUcx7l48aLTqlUrJyoqSj1nM2fO7CxYsOCO9w1nXh/X0aNHO127dnVmzpzpzJ071+nWrZuTLl06p2TJks65c+fueP9w5fVx1Q0fPtwREefAgQOJul84iqRxvalDhw5O2rRpnT179iTp/uHA6+Maqa/DjuPtsT19+rQjIk6uXLmcrFmzOsOHD3c+/fRTp0GDBo6IOBMmTPB7/3Dm5XE9ceKEExUV5XTo0MH4+e7du9Xn45MnT/o9Rrjy8rjaUvuzU8hd8ZQ2bVrJkCGDiPx5tcnp06fl+vXrUqlSJdm2bdstt2/atKkULFhQtatUqSJVq1aVJUuWiIjI6dOnZeXKldKqVSu5cOGCnDx5Uk6ePCmnTp2S+vXry969e/0WwqtTp444jiODBg0K+Hf46KOPZNmyZTJu3DgpW7asXL58WW7cuBHw/b0oXMf18uXLIiLGUqybbhbZu3mbSBWuYyvy57iWKlVKWrRoIbNmzZIZM2ZIpUqVpG3btrJx48ZEPhLeEs7j2q1bN3n//felTZs20rx5cxk9erRMmzZN9u7dK+PGjUvkI+Et4Tyu8M1L4/rJJ5/I1KlTpWfPnlKyZMlE399LwnlceR32L1zH9uayulOnTsmUKVOkV69e0qpVK1m8eLGUK1dOhgwZktiHwlPCdVxz584trVq1kmnTpsnIkSNl//79snbtWomLi5P06dOLSGR/3wnXcQ01ITfxJCIybdo0ueeeeyRjxoySK1cuyZMnjyxevFjOnTt3y21v96GkVKlSapvAn3/+WRzHkddee03y5Mlj/Bs4cKCIiPz+++9B7X/16tWlfv368uKLL8ry5ctlxowZ8uqrrwb1HOEoHMf15iWnNy9B1V25csW4TSQLx7EVEenatassXLhQZs+eLa1bt5Ynn3xSVqxYIQUKFJBu3boF5RzhLFzH9XbatGkj+fPnlxUrVrh2jnDhpXHF/3hhXNeuXSsdOnSQ+vXry9ChQ4N+/HDkhXG9iddhUziO7c3PvOnTp5cWLVqon6dJk0bi4uLkyJEjcujQoWSfJ5yF47iKiEycOFH+9re/Sa9eveTuu++WWrVqSYUKFeTxxx8XETF2p4xE4TquoSTkdrWbMWOGPP3009K0aVPp3bu35M2bV9KmTStvvfWW7Nu3L9HHu7ketVevXlK/fv3b3qZEiRLJ6rM/OXLkkLp168rMmTNlxIgRrp0n1IXruObMmVOio6Pl2LFjt+Ru/iw2NjbZ5wln4Tq28fHxMnXqVOnTp4+kSfO/Ofj06dNLw4YNZezYsRIfH6/+hyPShOu4+lO4cGE5ffq0q+cIdV4cV3hjXLdv3y5NmjSR8uXLy9y5cyVdupD7iJrivDCuNl6H/xSuY3uzCHJMTIykTZvWyOXNm1dERM6cOSNFihRJ9rnCUbiOq4hI9uzZ5fPPP5dDhw7JwYMHpWjRolK0aFGpUaOG5MmTR2JiYoJynnAUzuMaSkLuXX3u3LlSvHhxmTdvnlFV/+bsn23v3r23/GzPnj1SrFgxEfmzqKHIn18mH3nkkeB3OACXL1++7WxoJAnXcU2TJo1UqFBBtm7dektu06ZNUrx48ZDZKSC1hOvYnjp1Sq5fv37bZbDXrl2ThISEiF4iG67j6ovjOHLw4EG57777UvzcocRr44o/hfu47tu3Txo0aCB58+aVJUuWRPz/rN8U7uNq43X4f8J1bNOkSSMVK1aULVu23PKfc7/++quIiOTJk8e184e6cB1XXZEiRdTE4dmzZ+U///mPNG/ePEXOHaq8MK6hIOSW2t2cPXe0bVc3bdokGzZsuO3tFyxYYKyB3Lx5s2zatEkaNmwoIn/OvtepU0cmTpx426tWTpw44bc/idmu8HaXxB08eFC+/vrr2+6KFknCeVxbtGghW7ZsMSaffvrpJ1m5cqW0bNnyjvf3unAd27x580pMTIzMnz9f4uPj1c8vXrwoCxculDJlykT0MspwHVdfxxo/frycOHFCGjRocMf7e1k4jyt8C+dxPX78uDz22GOSJk0aWb58eUR/abWF87jyOuxfOI9tXFyc3LhxQ6ZNm6Z+duXKFZk5c6aUK1cuolcChPO43s6rr74q169fl+7duyfp/l7htXFNLalyxdOHH34oy5Ytu+Xn3bp1k8aNG8u8efOkWbNm0qhRIzlw4IBMmDBBypUrpwra6UqUKCE1a9aUF198Ua5evSqjR4+WXLlyGVuhf/DBB1KzZk2pUKGCPPfcc1K8eHH57bffZMOGDXLkyBHZvn27z75u3rxZHn74YRk4cOAdC3hVqFBB6tWrJxUrVpQcOXLI3r17ZerUqXLt2jUZNmxY4A9QmPLquHbu3FkmT54sjRo1kl69ekn69Onl3XfflXz58knPnj0Df4DCmBfHNm3atNKrVy/p37+/VKtWTdq1ayc3btyQqVOnypEjR2TGjBmJe5DCkBfHVUSkaNGiEhcXJxUqVJCMGTPKunXrZPbs2VKxYkXp1KlT4A9QmPLquJ47d07ef/99ERFZv369iIiMHTtWYmJiJCYmRrp27RrIwxO2vDquDRo0kP3790ufPn1k3bp1sm7dOpXLly+fPProowE8OuHLq+Ma6a/DIt4d206dOsmUKVOkS5cusmfPHilSpIhMnz5dfvnlF1m4cGHgD1CY8uq4Dhs2THbu3ClVq1aVdOnSyYIFC+TLL7+UIUOGSOXKlQN/gMKUV8c1pD47ub9x3v/c3K7Q17/Dhw87CQkJzptvvukULVrUiY6Odu677z5n0aJFTvv27Z2iRYuqY93crnD48OHOyJEjncKFCzvR0dHOQw895Gzfvv2Wc+/bt89p166dkz9/fid9+vROwYIFncaNGztz585Vt0nudoUDBw50KlWq5OTIkcNJly6dExsb67Ru3dr5/vvvk/OwhTyvj6vjOM7hw4edFi1aOHfddZeTNWtWp3Hjxs7evXuT+pCFjUgY25kzZzpVqlRxYmJinEyZMjlVq1Y1zuFFXh/Xjh07OuXKlXOyZcvmpE+f3ilRooTTt29f5/z588l52EKe18f1Zp9u90/vu9d4fVz9/W61a9dOxiMX2rw+rpH6Ouw43h9bx3Gc3377zWnfvr2TM2dOJzo62qlataqzbNmypD5kYcHr47po0SKnSpUqTrZs2ZzMmTM71apVc+bMmZOchywseH1cQ+mzU5TjaNeMAQAAAAAAAEEScjWeAAAAAAAA4A1MPAEAAAAAAMAVTDwBAAAAAADAFUw8AQAAAAAAwBVMPAEAAAAAAMAVTDwBAAAAAADAFekCvWFUVJSb/UAiOI4TtGMxrqGDcfWmYI6rCGMbSnjOehPj6k2MqzfxHutdPGe9iXH1pkDGlSueAAAAAAAA4AomngAAAAAAAOAKJp4AAAAAAADgCiaeAAAAAAAA4AomngAAAAAAAOAKJp4AAAAAAADginSp3QEAAAAAQMrKnj270T527JiKM2XKZOT69eun4mHDhhm5QLZSBxDZuOIJAAAAAAAArmDiCQAAAAAAAK5g4gkAAAAAAACuoMYTAAAAAESYnj17Gu3o6GgVJyQkGLkaNWqoOCoqyshR4wnAnXDFEwAAAAAAAFzBxBMAAAAAAABcEeUEeG2kfUllqKpYsaLRfvbZZ1VcrVo1I1e5cmWfxzl+/LjRLlWqlIovXLiQjB4mXzAvZw3lcc2XL5+KX3nlFSPXrVs3Fe/bt8/IxcXFGe1t27a50Lvgi5RxjTTBvvycsQ0dPGe9iXE1pUlj/h/ltGnTjHbbtm193nfGjBkq7tOnj5HTt21PCYyrN/Eem3hVq1ZV8erVq41chgwZfN7vwQcfVPHGjRuD3i8bz1lvYlxN+fPnN9q//vqr0dZ/xzFjxhg5/ftwagtkXLniCQAAAAAAAK5g4gkAAAAAAACuYOIJAAAAAAAArvBEjafy5cureOnSpUYuNjY2KOfYvn27inv06GHk7PXRbvPq2lh97biIyAcffKDie+65x+f9zp8/b7T1rWBFRIYMGaLioUOHJqeLrgrFcdVre2TJksXINWvWTMWlS5f2eYyyZcsa7aZNm6p469atRu6rr75S8bBhw4zc1atXjXZ8fLzPc4aScKg/Yde/W79+vc/b6n8Tr7/+upE7d+6cii9evGjkJk+enJwuhqRQfM6Gkk8++cRob968WcWjR49O4d4EjnEVyZUrl4rfeecdI/fMM88k6Zh2rUb7uG5jXL0pHN5jU1uxYsWMtv5Zq3jx4j7v9+233xrtJk2aqPjMmTPB6ZwfPGe9iXE1vffee0a7a9euPm9r15kuV66ciu3aUCmNGk8AAAAAAABINUw8AQAAAAAAwBWeWGr32muvqXjQoEE+b5eQkGC0Dx8+bLQ/++wzFb/88stGLl26dCqeOHGikevcuXPAfQ0Gr16i2Lt3b6P99ttvq/jGjRtGrn///irWx01E5McffzTa+pIse4nAnDlzktZZF4TCuOrLK0TMMUjq8oqksn+Hb775xmdb76eIyOXLl93rWCKFwzKAJUuWGO3HHnssoPP7+93s5+zx48dVvGbNGiP3xRdf+DyOPZaLFi3yeduUFgrP2VBibwm8bds2o71lyxYVP/HEEynSp6SIxHF96qmnjLb+fqyXM0iOf//730Y7Li4uKMcNVCSOayQIh/fY1DZ9+nSj3aZNG5+3vXLliorr1q1r5DZt2hTcjt1BJD5n7bIjJUuWVHGVKlWMnL1MUv/sdvLkSSOnv/526dIl2f1MjkgcV5v+vvr5558bOXtprD8jRoxQcd++fZPdr+RgqR0AAAAAAABSDRNPAAAAAAAAcAUTTwAAAAAAAHBFujvfxD0ZM2Y02mnTplXxH3/8EfBxvv76axW3bNnSyOlrWhcvXmzk7PoTuhdeeMFo6zWe4A5/a871bbhFbq3po2vevLnRnjVrlopnzJhh5A4dOqTijRs3BtRPLxsyZIjRDrSuk72F56VLl5LdF3vdduHChY22XtutVKlSRq5Pnz4+j6uvbc+bN6/P2+l1xEREjh075ruzYS579uxBP6b+ei4iUrBgQRXbz/V//OMfPo+j12gTEfnvf/+rYvtvRK8bdOLEicA7i6CwazzZbaQuu4afXhuibdu2Rk7/zGPXbbh69arRXrVqlYrtMa9YsaKKq1evbuT02+o14BCZ9PehZs2aGbnVq1er+ODBgynUo/A1dOhQo+3vPdamvz+ndE2nSGG/Tg4cOFDF7dq1M3L6d2X7M4/9Oefo0aMqtj/f6t9r9e9FIiLr1q0LpNsIoqVLl6o4NjY24Pv5+0wcDrjiCQAAAAAAAK5g4gkAAAAAAACuSNX1Y/ZSjAwZMqg4MUvtvv32WxXfc889ye8YUoy+naS/Sw0zZ85stKOjo1VsX/Zvb83+5JNPqtjeUla/pHXfvn1GLhKX6nTq1Mlo60ss7Mf5zTffVPGUKVOMnBvLJjJlymS0x40bp+ImTZoYuXr16qnYvjTZXm7iy/bt2432mDFjArofEs9emnnq1Cmft926dauK165da+QuXrwY3I4hUextoG2///57CvUEN9WqVUvFY8eONXL6+68tISFBxcuXLzdyw4YNM9pr1qxRsb58wFaoUCGjnTVrVp+3hTfUrFnTaOfLl0/F+t+miEhcXJyK8+TJY+T013Y3loZ7QdWqVVXcsWNHI+dvy/n58+cbbbssCYJD/9v/8MMPjVz9+vVVbH/2/Pzzz28bi5jlQkRELl++rOLKlSsbOf212X5estQu5SW1hE+aNOY1Qzly5AhGd1IMVzwBAAAAAADAFUw8AQAAAAAAwBVMPAEAAAAAAMAVqVrjya7jlJi6TsFg15jStxO268nAHc8884yK7do7Fy5cUHHu3LmN3F133aXiO9ViWrBggYr1+jAi5vai9nrb559/3u9xvejw4cNGW6/JcfLkSSM3ceJEFadEPSx97bp9fn19vIi5lt6m/x67d+82cnodJ3stPdxj1xazt/pFeChZsqTf/JIlS1KoJ7hJfy30V9PJrnGov76OGDEi+B1DqitRooTR7t+/v9HW/3b81eO6U223pLA/i8yYMSPo5wh3999/v9HWP7PYn5ltO3bsUPHTTz9t5K5fv578zuEWvXv3VrH9mVX/++7bt6+RS2rNVL32nohZp7VLly5Gzq7bh9Blf1ctVqxY6nQkibjiCQAAAAAAAK5g4gkAAAAAAACuSNWldsGiX3bm71JyW/v27Y32yy+/HND9UmJZUSSaO3eu0R49erSKp06dauT0S8ATMx72kh59qVWrVq2MnL7cZ9WqVQGfI5wNHjzYaE+aNEnFBQsWNHKrV69W8XfffWfkRo0apWJ7eWNSValSxWjrWwDnzZvXyJ05c0bFn3zyiZEbN26ciu2ldpHKviS7WrVqPm+rb+Wqb7meGPZ2sPYyiqFDh6r4/fffN3LffPONirdt25ak8wORQl+ypG9JL2Iun3r99deNXGKWNunbOefPn9/n7Y4cOWK07f4g+F566SWj/eijj6o4S5YsRq5OnTo+jxMVFWW0Hce5bWyzx3jZsmUq3rVrl5HTX9u3bNli5C5duuTzHJEkLi5OxVOmTDFymTNn9nk//TORiEjHjh1VbI+R/v6cPXv2gPt27do1n8eMRPZSSP25qJcAERF59tlnVXzjxg1X+qN/XipQoIAr54BJf43Vv0+JiOTJkyeluxMSuOIJAAAAAAAArmDiCQAAAAAAAK5g4gkAAAAAAACuCJsaT3p9l169ehm5woUL3/Z2yWGvJ+/cubOKP/vss6CcA2YtmQ0bNhi5jRs3qtiuN6HXeNq5c2fA59u7d6/Rnjdvnor//ve/Gzl9i9lIqfH08ccfG+1ffvlFxcuXLzdyZcqUuW0sItKoUSMV79+/38hNmDBBxf/617+M3JUrV1Rs16awt3r2t12wXi8s0NptkUzfhlnE3PbXptd18lfbwx+7NpR9nCJFiqh4+PDhRu7cuXMqfvHFF43cwoULVXz58uUk9Q3wEv19tEKFCkYuffr0KrZfpxOjaNGiKr733nt93s5+j0/qNuHwb8CAASoeNGiQkTt58qSK7dfI+Ph4o+2vhp7+ueuHH34wcps2bVLx999/b+So1ZQ4VatWNdrvvfeeiv3VdDp16pTR7tu3r9HWa2/atWb0uootW7YMuK+//vqrihs3bmzktm/fHvBxvKJ79+5GW39+2fVm3arrpHvggQdUbNeYgjuGDBmi4tatWwflmIcOHTLakydPDspxUwpXPAEAAAAAAMAVTDwBAAAAAADAFSG71M7eulVfdtO8eXPXz29f9qhv7cqlwkmXKVMmo122bFkV25fh6956662gnN9e0jNs2DAV16pVy8jpS+/effddI+fVy4btv/vVq1er2L7k+4UXXlDxY489ZuQKFiyoYnvpxfjx41VsX/69du1aFTdp0sTI2dv6nj9/XsVPPfWUkVu5cqXAHfrl9AcPHgz4fsWKFVNxbGxsks+v/x3MmjXLyOlLQ+wlJosXL07yOQEv0JdOB1PlypUDup2+vAfJo78O6mUBRERee+01n/c7cOCAiu2ldfZrpF5iAimnYsWKKv7iiy+MnL8SA/ryOvvzU6tWrYy2vgTXXmqnv1cnhv6+vmzZMiNXoECBJB0z3OhjZz/merkKfcmrWzJmzGi09e/VSS2TgMQpV65c0I9pl8L4+eefg34ON3HFEwAAAAAAAFzBxBMAAAAAAABcwcQTAAAAAAAAXBGyNZ7sbULbtGmTpOPoa5737dtn5LJmzWq09bWY2bJlM3J16tRR8e7du5PUF4jUq1fPaMfExKjYHp+UoNecmD59upHTt0Lt16+fkYuLi3O3YyHi+vXrKrbrc3Ts2FHFOXLk8JnTnzsiIg0aNFCxXUtAb9t13v744w+jrdfgWrVq1W16j2A4e/as0da3SU5MrTO91lfJkiX93lav6XffffcZOX9bSOu3HThwoJHT/0ao0+cO+/UdkeHRRx/1mdPf12fMmJES3fGkdOnMj+tffvmliitVqhTwcfzdNm/evEY7f/78Kj5+/HjA50DiREdHG+3XX39dxf5qOtmuXr2q4o8//tjI3ek9N9j0WmKRRP9OYz9nd+zYkaJ9sWtM6d9rz507l6J9QfLo9XYXLVqUeh0JAq54AgAAAAAAgCuYeAIAAAAAAIArQnap3ZUrV4x2//79VVymTBkjp19eOmXKFCOnb1m5f/9+I2cvp9OXWj3++ONGbtiwYSq2tyReunTprb8AbsvflqqpfSm3v6V28O/MmTNGe/jw4SoeNWqUkdOX4X3wwQcBnyNTpkxG+/nnn1dxtWrVjNycOXNUnBpLOL1kyJAhRjsxy+t83e9Ox5g7d66KGzVqZOR69OihYnsZp+7+++832uPHj1dx+/bt/Z4fSVOkSBGjnZCQYLTtZZsIHRkyZDDa+lJnOzdu3Dij3bJlS5/HnTx5soqPHTuWnC5GtCxZshjtypUrq/jw4cNG7vz58yrWl+TZWrdubbTt5VhvvPGGip977rnAO4s70sdz6tSpRk5fzp4YsbGxAd9W/xuxl4B9++23Kt62bZuRe+SRR1TcoUMHn8fX/3bwJ3/ff5IqY8aMRnv58uUqtj8X6z777LOg9wXuiY+PV7E9PxJuuOIJAAAAAAAArmDiCQAAAAAAAK5g4gkAAAAAAACuCNkaTzdu3DDaei0Ye02rXsfJrinhz4ULF4y2vvXkqVOnjJxeD8qu/0SNp8Dp68NDjV1/gnoUwXH9+nWfOb2OiO2PP/4w2nadkbi4OBXbtSpeeOEFFQ8ePNjI6dsM268zkWrjxo1G294GODUtXrzYaOt9S5s2rZF76KGHfB5Hr5th13+y61ggcOnTp1ex/Xy+ePGi0V61alWK9An/o9fG07f6FhF5+eWXVVy7dm0jp7/e2s8Xm+M4PnN6vRgkTsOGDVVsP3f0emqnT582cpcuXfJ5TL3uS+fOnY2c/Znn2WefVTE1npLHfq/66KOPVNy8eXPXz2/XhWnTpo2K/X2HqVevntHWa4v5s3fv3kT0LjLoj3nu3LmNnP491h/9/Vbk1u+jdu1inf45Z926dQGdD4lTrlw5o/2Xv/wllXoSurjiCQAAAAAAAK5g4gkAAAAAAACuYOIJAAAAAAAArgidQh5irnl9++23jVydOnVUnDVrViNXunRpFZ89ezbJ54+Pj0/yfZF8+/fvT9Xz//bbb0b7999/T6WeeFuPHj1U7K82iL1W3a5Borf79u1r5AoVKqTiSZMmGbm6deuq+P/+7/+M3MGDB332B6Hh888/V/HXX39t5H788UcVFyhQwMhlz55dxbVq1TJy1HhKuiZNmqjYfm/etWtXSncn4tmvkwMHDlSx/jnqTvR6Xf5ep+Eevb7d+vXrjdzEiRNVbNfoKVWqlIpr1qxp5Jo1a6bitWvXGrn8+fMbbbvOIpKuS5cuRtvtuk52DZ/XXnvNaK9Zs8bnfVu2bKniN954w8iVLFlSxXaNTP21JrU/z6eW48ePq3jfvn1GTn/s9JqTiWHXUbRrd129elXFdj3kd999N0nnROD++te/Gu277747KMf94YcfgnKcUMAVTwAAAAAAAHAFE08AAAAAAABwRaoutcuRI4fRnjdvnooffPBBI6dviT5t2jQjl5zldb4MGjTIaL/zzjtBPwdM9hKB7du3p+j57UvSixYtquI9e/akaF9wq2+++cZn214yMGDAABXby6pat26t4kqVKhm5+vXrq9hry+5iY2ONtr59b7guM7548aLRTkhISKWeRC77OaRbsWJFCvYkcmTOnNloP/LIIyqeOnWqkcuVK5fP4+zcuVPF9jbQ9vbvSF01atTw2b506ZKR05fY2OOoL5u0ywvky5fPaLdt2zZpnYWImOVD9OVrwXL9+nWjrS9105diity6bPLJJ59U8d///ncj9/jjj6vY3+uA/jlLRGTYsGF36LH37d69W8WdOnUycvrj/Le//S3gY+qlAOwljJMnTzba48aNU7H9PVpfugt32GOeVPYSylGjRgXluKGAK54AAAAAAADgCiaeAAAAAAAA4AomngAAAAAAAOCKVK3xZG+Brq9HtdcnDx06VMX2uuZg0Wsc9O/f3+ftZs6c6cr5I8GZM2d85ooUKZKCPblVhw4djHZMTIyKT58+ncK98Y5q1aoZbXvLZt38+fOTdI5Vq1YZbb12yaeffmrk9FpiJUqUMHL//Oc/bxt7QVxcnNGeM2eOio8ePZrS3QmKbNmyGe00afi/lJRmbx8M9+k1nUREFixY4PO2ei03+7WwV69eKm7Tpo2R69ixo4qrV6+elG6KiEjhwoWTfN9It379ehXbNZ50ds2vQNk1nTZt2mS0v/zyyyQdF38aM2aMiv2NX1LpNSlFRMqUKaPif//730auYMGCRrtUqVIBncP+G1i0aJGK9XpCuJX9uVRvv/TSS0E5h/49RUSkfPnyKt64caORO3/+fFDOCVOdOnVUnJz3Sp3+2i/irbHjUzoAAAAAAABcwcQTAAAAAAAAXJGqS+1mzJhhtHv37q3i9u3bG7k33nhDxceOHXOlP2fPnlVxdHS0z9s9/PDDRtu+JA6+6Zfpiog8//zzKra3c3ZDVFSU0X755ZdVrG8vK2L+Pbz//vuu9svLMmXKZLT9bc977733BuWcJ06cULG9xExfbqIvuxMRiY2NVbHdzxs3bgSlb0ieJ554QsX2csgCBQqkcG/gzzfffJPaXfCMQoUKqXjq1Kk+b2cvl3rllVdU7G88atasabT1pUGO4wTcT1vbtm1VPGvWrCQfJxK9+OKLKp40aZKRs5ew+2J/5tGXbNhLNPXllSLulbXwKnvpfrCW3fhiL4Pz99nKn19//dVo/+Mf/1Dxt99+a+QSEhKSdA6446GHHjLa+vLZgQMHpnR3IlKjRo1UnDFjxqAcs169ekZb/w5qz4+EG654AgAAAAAAgCuYeAIAAAAAAIArmHgCAAAAAACAK1K1xpM/9jrJnj17qljfAjg57HogixcvVrFd40lfA/3hhx8G5fyRyK7PFR8fr+K6desauZYtW6rY3ho2MRo3bnzbY4qIPPXUUz7vN2jQIBX/8MMPST5/pLO3lF2zZo2KH3vsMSNXvHhxFet13URurWty8ODBgM6v13sSERkwYICKly5dauSaN2+u4ueee87ITZgwIaDzhQv995s4caKRc6uOXqD0ujRDhw71ebs0acz/O/FXf+LIkSMqHj16dNI7B5927dpltPX3VCSP/pkoV65cPm+n18oUEVm3bp2KixUrZuSmT5+uYn/1aK5cuWK0X331VaOtfz7Ta1EheXbu3KniRx55xMhVrlxZxXZtId3FixeN9ooVK1R86tSp5HYRmq+++spoFylSxNXzJaam08qVK422/tr80UcfGblz584lr2NIMV27dvWZs2uAIXzlzJkztbsQNFzxBAAAAAAAAFcw8QQAAAAAAABXRDkB7pNrb8kaDKVKlTLa+la/efPm9Xk/e9mTvhRj48aNRk7fSr1o0aJGzt46Vr9s1d46vUmTJipetmyZz76lhORsbWxzY1wTY+HChSrWt6QUMcfAvhRYz919991Gzt4WWl826e/3tZf06FuRpsQWsl4aV3/0ZSLz5883cg8++KCK7d9h9+7dRltf3mEvmQvUnj17jLb+t2RvNa0vw0uMYI6rSNLH1n5+zZ49W8X2cozff/9dxUOGDEnS+Wz6a2jt2rWNnP0Y5cmTR8UZMmTweUz7sdCPYy+J1pdYHj9+PIAe31mkPGd1MTExRnvfvn0q/uWXX4zc/fffnxJdCrpQHFd9OZX9uqU7evSo0daXyd11111GTn+e2S5fvqziv/71r0bOXuY8Z84cFbdo0cLIvf322yq2l+iltFAcVyRfqLzH2t9pAv3caC+VbNiwYUD3u3r1qtGeNGmSz9seOHDAaNvfcUIVz1mT/Xnou+++M9r6d+cyZcoYuZMnT7rWr8Ty0rgOHz5cxT169HDlHH/5y19UfOjQIVfOEQyBjCtXPAEAAAAAAMAVTDwBAAAAAADAFUw8AQAAAAAAwBWpWuPJptd3sbcl1ev0pIT+/fsb7bfeeitFz++Pl9bGli5dWsWrV682cvny5UvSMe3fKT4+XsVr1qwxcu+8846K7e1mU3oNvJfGNVB2vRh9i197e2/78bl27ZqKk7rm2d7uOH369Cr+/vvvjdx9992XpHOESv0Jm15fq06dOkYuW7ZsAZ0/qb+bv9pMyTnO1KlTVWyvtbfrWAVDJD5nc+fObbT1emBjxowxcv/85z9ToktBF4rjmi5dOhX369fPyA0aNCjZx9+6davR1j/z2LX4bPr7eOfOnY3ce++9p+L9+/cnp4vJForjiuQL1fdYJB/PWdMDDzxgtDdv3my09XpDr7zySor0KSm8NK7BqPFkv/926NDBaOt1HfXvtKGGGk8AAAAAAABINUw8AQAAAAAAwBXp7nyTlLN+/XoVly9f3sjVr19fxfZl5rGxsQEd/9KlS0bb3m77008/VfHGjRsDOiaS56efflKxvdVy+/btVWxvv/7HH3+oeO7cuUbOvuxy2rRpKl61alXSO4ugO3v2rNHWn+eNGzc2cn379jXa99xzj4rvvvvuoPRn+/btKh4wYEBQjhmqmjVrpuJnnnnGyOnLYzJnzpxifbpJ3yZaX8pl6927t9HWl2rq28HDPfrrrb1lN4Ln+vXrKp4yZYqR07djb9u2rc9jTJgwwWifOXNGxW+++aaR099j70R/H+/WrVvA9wMABO6ll17ym581a1YK9QQ36d9Bs2TJYuQ6derk8369evVS8WeffWbkklo+JBxwxRMAAAAAAABcwcQTAAAAAAAAXMHEEwAAAAAAAFwR5QS4p2Fqb1eI//HSNpT4H8bVv+joaKOt1/3Kly+fkStVqpSK27VrZ+S+/PJLFR8/ftzIjRo1SsWnT59Oemc14bjVc6NGjVRcsmRJIzdy5EgVJ/V3s3+HwYMHG+3du3erWK+9F2oi8TmbLVs2o71r1y4V2/UMli5dmiJ9CrZIHNdIwLh6Uzi+xyIwPGdNe/fuNdp67T8RkVq1aqn4xIkTKdKnpGBcvSmQceWKJwAAAAAAALiCiScAAAAAAAC4gqV2YYhLFL2JcfUmlgF4F89Zb2JcvYlx9SbeY72L56xIx44dVTxp0iQjN3r0aKPdo0ePlOhSsjGu3sRSOwAAAAAAAKQaJp4AAAAAAADgCiaeAAAAAAAA4ApqPIUh1sZ6E+PqTdSf8C6es97EuHoT4+pNvMd6F89Zb2JcvYkaTwAAAAAAAEg1TDwBAAAAAADAFUw8AQAAAAAAwBVMPAEAAAAAAMAVTDwBAAAAAADAFUw8AQAAAAAAwBVRTrD3IQUAAAAAAACEK54AAAAAAADgEiaeAAAAAAAA4AomngAAAAAAAOAKJp4AAAAAAADgCiaeAAAAAAAA4AomngAAAAAAAOAKJp4AAAAAAADgCiaeAAAAAAAA4AomngAAAAAAAOCK/wfScvmd7+aINAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1500x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize some random images from the dataset\n",
        "def visualize_random_images(X_train, y_train, num_images=5):\n",
        "    # Generate random indices\n",
        "    random_indices = np.random.choice(len(X_train), num_images, replace=False)\n",
        "\n",
        "    # Create subplots to display images\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        # Get the image data and corresponding label\n",
        "        image = X_train[random_indices[i]]\n",
        "        label = y_train[random_indices[i]]\n",
        "\n",
        "        # Display the image\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title(f\"Label: {label}\")\n",
        "        ax.axis('off')  # Hide axis\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Visualize 10 random images from the training set\n",
        "visualize_random_images(X_train, y_train, num_images=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJwZZy5ROoGs"
      },
      "source": [
        "**Step 3: Implement the Simple Perceptron model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xaiHE_7OrzA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimplePerceptronSoftmax:\n",
        "    def __init__(self, input_size, num_classes, learning_rate=0.001, epochs=50):\n",
        "        self.weights = np.random.randn(input_size, num_classes)  # Initialize weights for multi-class\n",
        "        self.bias = np.random.randn(num_classes)  # Bias for each class\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def softmax(self, z):\n",
        "        \"\"\"Softmax activation function\"\"\"\n",
        "        exp_values = np.exp(z - np.max(z))  # Subtract max for numerical stability\n",
        "        return exp_values / np.sum(exp_values, axis=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Compute the output of the perceptron (forward propagation with softmax)\"\"\"\n",
        "        z = np.dot(x, self.weights) + self.bias\n",
        "        return self.softmax(z)\n",
        "\n",
        "    def compute_loss(self, y, output):\n",
        "        \"\"\"Compute cross-entropy loss\"\"\"\n",
        "        return -np.sum(y * np.log(output))\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        \"\"\"Update weights and biases using backward propagation\"\"\"\n",
        "        error = output - y\n",
        "        dw = np.dot(X.T, error)  # Gradient for weights\n",
        "        db = np.sum(error, axis=0)  # Gradient for bias\n",
        "        # Update weights and bias using the learning rate\n",
        "        self.weights -= self.learning_rate * dw\n",
        "        self.bias -= self.learning_rate * db\n",
        "        return error\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"Train the perceptron over multiple epochs\"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "            for x, target in zip(X, y):\n",
        "                output = self.forward(x)\n",
        "                loss = self.compute_loss(target, output)\n",
        "                total_loss += loss\n",
        "                # Backpropagation to update weights and bias\n",
        "                self.backward(x.reshape(1, -1), target.reshape(1, -1), output)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}: Total loss = {total_loss}\")\n",
        "\n",
        "            # Early stop if loss is minimal\n",
        "            if total_loss < 1e-6:\n",
        "                print(\"Training complete!\")\n",
        "                break\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the class for each input\"\"\"\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)  # Return the index of the max probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycC4ip6cQRvV"
      },
      "source": [
        "**Step 4: Train the Simple Perceptron model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vSoAzFGREVl",
        "outputId": "132e1803-110b-48e1-d8a1-146dd317a181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Total loss = 8559093.636305084\n",
            "Epoch 2: Total loss = 5919691.962554076\n",
            "Epoch 3: Total loss = 5448691.646939118\n",
            "Epoch 4: Total loss = 5252647.088505608\n",
            "Epoch 5: Total loss = 5151631.714526476\n",
            "Epoch 6: Total loss = 5092633.441465101\n",
            "Epoch 7: Total loss = 5055063.778796093\n",
            "Epoch 8: Total loss = 5029550.150463552\n",
            "Epoch 9: Total loss = 5011331.2505714595\n",
            "Epoch 10: Total loss = 4997785.206428603\n",
            "Epoch 11: Total loss = 4987375.365411837\n",
            "Epoch 12: Total loss = 4979154.394374985\n",
            "Epoch 13: Total loss = 4972512.859588726\n",
            "Epoch 14: Total loss = 4967044.032675868\n",
            "Epoch 15: Total loss = 4962467.6185352765\n",
            "Epoch 16: Total loss = 4958584.933543969\n",
            "Epoch 17: Total loss = 4955251.60579687\n",
            "Epoch 18: Total loss = 4952360.40619967\n",
            "Epoch 19: Total loss = 4949830.133796089\n",
            "Epoch 20: Total loss = 4947598.229350279\n",
            "Epoch 21: Total loss = 4945615.748694155\n",
            "Epoch 22: Total loss = 4943843.867707368\n",
            "Epoch 23: Total loss = 4942251.404581899\n",
            "Epoch 24: Total loss = 4940813.032073483\n",
            "Epoch 25: Total loss = 4939507.966826197\n",
            "Epoch 26: Total loss = 4938318.994365317\n",
            "Epoch 27: Total loss = 4937231.734077086\n",
            "Epoch 28: Total loss = 4936234.078289447\n",
            "Epoch 29: Total loss = 4935315.759364584\n",
            "Epoch 30: Total loss = 4934468.012095847\n",
            "Epoch 31: Total loss = 4933683.307859286\n",
            "Epoch 32: Total loss = 4932955.143387917\n",
            "Epoch 33: Total loss = 4932277.871506563\n",
            "Epoch 34: Total loss = 4931646.564450769\n",
            "Epoch 35: Total loss = 4931056.902645014\n",
            "Epoch 36: Total loss = 4930505.083601717\n",
            "Epoch 37: Total loss = 4929987.746776371\n",
            "Epoch 38: Total loss = 4929501.9112387635\n",
            "Epoch 39: Total loss = 4929044.923642448\n",
            "Epoch 40: Total loss = 4928614.414565973\n",
            "Epoch 41: Total loss = 4928208.261692516\n",
            "Epoch 42: Total loss = 4927824.558587773\n",
            "Epoch 43: Total loss = 4927461.58812011\n",
            "Epoch 44: Total loss = 4927117.799717181\n",
            "Epoch 45: Total loss = 4926791.789831779\n",
            "Epoch 46: Total loss = 4926482.285100051\n",
            "Epoch 47: Total loss = 4926188.127755536\n",
            "Epoch 48: Total loss = 4925908.262966438\n",
            "Epoch 49: Total loss = 4925641.727787913\n",
            "Epoch 50: Total loss = 4925387.641513001\n",
            "Epoch 51: Total loss = 4925145.197200402\n",
            "Epoch 52: Total loss = 4924913.65422453\n",
            "Epoch 53: Total loss = 4924692.331708111\n",
            "Epoch 54: Total loss = 4924480.60271003\n",
            "Epoch 55: Total loss = 4924277.889069987\n",
            "Epoch 56: Total loss = 4924083.656837957\n",
            "Epoch 57: Total loss = 4923897.412196084\n",
            "Epoch 58: Total loss = 4923718.697812359\n",
            "Epoch 59: Total loss = 4923547.08959852\n",
            "Epoch 60: Total loss = 4923382.193784597\n",
            "Epoch 61: Total loss = 4923223.644306605\n",
            "Epoch 62: Total loss = 4923071.100448872\n",
            "Epoch 63: Total loss = 4922924.2447296595\n",
            "Epoch 64: Total loss = 4922782.780977092\n",
            "Epoch 65: Total loss = 4922646.432608824\n",
            "Epoch 66: Total loss = 4922514.941058869\n",
            "Epoch 67: Total loss = 4922388.064362901\n",
            "Epoch 68: Total loss = 4922265.575866094\n",
            "Epoch 69: Total loss = 4922147.263054247\n",
            "Epoch 70: Total loss = 4922032.926481371\n",
            "Epoch 71: Total loss = 4921922.378803801\n",
            "Epoch 72: Total loss = 4921815.443886312\n",
            "Epoch 73: Total loss = 4921711.955994945\n",
            "Epoch 74: Total loss = 4921611.759048417\n",
            "Epoch 75: Total loss = 4921514.705945189\n",
            "Epoch 76: Total loss = 4921420.657933151\n",
            "Epoch 77: Total loss = 4921329.484041593\n",
            "Epoch 78: Total loss = 4921241.060544742\n",
            "Epoch 79: Total loss = 4921155.270488344\n",
            "Epoch 80: Total loss = 4921072.003235768\n",
            "Epoch 81: Total loss = 4920991.154055397\n",
            "Epoch 82: Total loss = 4920912.62374054\n",
            "Epoch 83: Total loss = 4920836.318263466\n",
            "Epoch 84: Total loss = 4920762.148437675\n",
            "Epoch 85: Total loss = 4920690.029629603\n",
            "Epoch 86: Total loss = 4920619.8814679645\n",
            "Epoch 87: Total loss = 4920551.627594146\n",
            "Epoch 88: Total loss = 4920485.195410361\n",
            "Epoch 89: Total loss = 4920420.515867552\n",
            "Epoch 90: Total loss = 4920357.523248496\n",
            "Epoch 91: Total loss = 4920296.154977124\n",
            "Epoch 92: Total loss = 4920236.351438464\n",
            "Epoch 93: Total loss = 4920178.055808438\n",
            "Epoch 94: Total loss = 4920121.213896531\n",
            "Epoch 95: Total loss = 4920065.774003991\n",
            "Epoch 96: Total loss = 4920011.686775838\n",
            "Epoch 97: Total loss = 4919958.905081448\n",
            "Epoch 98: Total loss = 4919907.383894116\n",
            "Epoch 99: Total loss = 4919857.080170655\n",
            "Epoch 100: Total loss = 4919807.952757458\n"
          ]
        }
      ],
      "source": [
        "# Initialize and train the perceptron model\n",
        "perceptron = SimplePerceptronSoftmax(input_size=X_train_flattened.shape[1], num_classes=10, learning_rate=0.01, epochs=100)\n",
        "perceptron.train(X_train_split, y_train_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qhibe12RODI"
      },
      "source": [
        "**Step 5: Evaluate the performance of the Simple Perceptron model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr8TDLR4RNH0",
        "outputId": "bbf7ec90-6fc4-4227-d0e7-612fc59f3a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing the perceptron:\n",
            "Test Accuracy: 9.80%\n"
          ]
        }
      ],
      "source": [
        "# Test the perceptron after training\n",
        "print(\"\\nTesting the perceptron:\")\n",
        "predictions = perceptron.predict(X_test_flattened)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_test == predictions) * 100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GBZUGOVTlnd"
      },
      "source": [
        "**Step 6: Analysis on the results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR2WlMKTT21w"
      },
      "source": [
        "An accuracy of **9.80%** for the test set is **not satisfactory**, especially when using the MNIST dataset. MNIST is a relatively simple dataset, and modern models should be able to achieve much higher accuracy (typically above 90% for simple models, and even higher with more advanced models like Convolutional Neural Networks). Here are a few ideas on how to improve the accuracy:\n",
        "\n",
        "### 1. **Model Improvements**:\n",
        "   - **Increase Model Complexity**:\n",
        "     The Simple Perceptron model used here is quite basic, and it's not well-suited for solving complex problems like MNIST, which requires a model capable of capturing spatial hierarchies in the data (like how numbers are written).\n",
        "     \n",
        "     - **Use Multi-Layer Perceptron (MLP)**: A simple perceptron with a single output layer is not sufficient for a complex dataset. Adding hidden layers (forming a Multi-Layer Perceptron) can help the model learn more complex representations.\n",
        "     \n",
        "     - **Try Convolutional Neural Networks (CNNs)**: CNNs are the go-to architecture for image classification tasks. They are specifically designed to work well with grid-like data such as images. Even a small CNN model will likely outperform a perceptron.\n",
        "\n",
        "### 2. **Hyperparameter Tuning**:\n",
        "   - **Learning Rate**: The learning rate might be too small. A learning rate that is too small can lead to very slow training and convergence, while one that is too large can cause overshooting of the optimal weights. It might help to experiment with larger learning rates like `0.1`.\n",
        "   \n",
        "   - **Number of Epochs**: The model might not be trained for enough epochs. \n",
        "\n",
        "### 3. **Regularization**:\n",
        "   - **L2 Regularization**: Adding regularization (like L2 regularization) can help prevent overfitting and improve generalization. This will reduce the impact of large weights and encourage the model to learn simpler, more robust features.\n",
        "\n",
        "### 4. **Data Augmentation**:\n",
        "   - The MNIST dataset has a fixed set of training images. We can apply **data augmentation** techniques such as rotation, scaling, shifting, and flipping to artificially expand the dataset and help the model generalize better. This is especially useful when the model is struggling to generalize well on unseen data.\n",
        "\n",
        "### 5. **Weight Initialization**:\n",
        "   - **Better Weight Initialization**: The perceptron uses random weight initialization which can sometimes lead to poor convergence. Using more advanced weight initialization techniques (like Xavier or He initialization) can help the network converge faster and more effectively.\n",
        "\n",
        "### 6. **Optimizer**:\n",
        "   - **Switch Optimizer**: The perceptron is using basic gradient descent, which might be slow and inefficient. We can switch to more advanced optimizers like **SGD with momentum** or **Adam**, which can lead to faster convergence and better performance.\n",
        "\n",
        "### 7. **Batching**:\n",
        "   - **Mini-batch Gradient Descent**: Instead of updating weights for each individual training sample (stochastic gradient descent), you can use mini-batches of data for each weight update. This tends to result in more stable updates and can speed up training.\n",
        "\n",
        "### 8. **Data Preprocessing**:\n",
        "   - **Better Normalization**: Although the pixel values have been normalized to a range of [0, 1], consider also applying **zero-centering** (subtracting the mean pixel value from each pixel) to help the model learn better.\n",
        "\n",
        "### 9. **Model Evaluation**:\n",
        "   - **Cross-Validation**: Instead of relying on a single split between training and testing data, using **cross-validation** can help ensure that the model is generalizing well across all subsets of the data.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "While the current Simple Perceptron model is not performing well (with only ~9.80% accuracy), switching to a more sophisticated model (like an MLP or CNN), tuning hyperparameters, adding regularization, or using advanced optimizers could significantly improve accuracy. Starting with these suggestions and iterating on the model will likely lead to a much better result on the MNIST dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
